# Humanware-block1

## Running the experiment
```
python split_data.py --train_pct 0.7 --valid_pct 0.2 --test_pct 0.1
python main.py --model [LargeCNN, MediumCNN, SmallCNN]
```
Note: 
1. the images need to be in `./data/SVNH/train/`
2. The model `LargeCNN` is the model describe in Goodfellow et al, 2013

## Data
```
cd Humanware-block1
cp '/rap/jvb-000-aa/COURS2019/etudiants/data/humanware/SVHN/train_metadata.pkl' './data/SVHN/'
cp '/rap/jvb-000-aa/COURS2019/etudiants/data/humanware/SVHN/train.tar.gz' './data/SVHN/'
tar -xzf $HOME'/digit-detection/data/SVHN/train.tar.gz' -C './data/SVHN/'
```

## Container
```
source /rap/jvb-000-aa/COURS2019/etudiants/common.env
echo 'source /rap/jvb-000-aa/COURS2019/etudiants/common.env' >> ~/.bashrc
singularity shell --nv /rap/jvb-000-aa/COURS2019/etudiants/ift6759.simg
```

## To do
-Find standardization coefficients for images (in self.transforms)

# accuracy for base model

epoch 0 loss 0.591140377132595 
epoch 1 loss 0.4636336426219695 
epoch 2 loss 0.41813828195151154 
epoch 3 loss 0.5821610296318468 
epoch 4 loss 0.47141574378928464 
epoch 5 loss 0.6411254115534272 
epoch 6 loss 0.7003891050583657 
epoch 7 loss 0.5695899431307991 
epoch 8 loss 0.7024842861418736 
epoch 9 loss 0.7144567494762047 
epoch 10 loss 0.6450164621370847 
epoch 11 loss 0.7294223286441185 
epoch 12 loss 0.7096677641424723 
epoch 13 loss 0.718048488476504 
epoch 14 loss 0.7378030529781503 
epoch 15 loss 0.7345106255612093 
epoch 16 loss 0.7473810236456151 
epoch 17 loss 0.7012870398084405 
epoch 18 loss 0.7072732714756061 
epoch 19 loss 0.7665369649805448 
epoch 20 loss 0.7117629452259803 
epoch 21 loss 0.7683328344806944 
epoch 22 loss 0.7515713858126309 
epoch 23 loss 0.7761149356480096 
epoch 24 loss 0.7342113139778509 
epoch 25 loss 0.7812032325651003 
epoch 26 loss 0.7835977252319665 
epoch 27 loss 0.6997904818916492 
epoch 28 loss 0.7871894642322658 
epoch 29 loss 0.796168811733014 
epoch 30 loss 0.761448668063454 
epoch 31 loss 0.7922777611493564 
epoch 32 loss 0.8099371445674948 
epoch 33 loss 0.7683328344806944 
epoch 34 loss 0.8042502244836875 
epoch 35 loss 0.8039509129003293 
epoch 36 loss 0.8051481592337624 
epoch 37 loss 0.7862915294821909 
epoch 38 loss 0.8018557318168213 
epoch 39 loss 0.7997605507333134 
epoch 40 loss 0.8153247530679437 
epoch 41 loss 0.8048488476504041 
epoch 42 loss 0.824304100568692 
epoch 43 loss 0.8204130499850344 
epoch 44 loss 0.818916492068243 
epoch 45 loss 0.815624064651302 
epoch 46 loss 0.7955701885662975 
epoch 47 loss 0.8269979048189164 
epoch 48 loss 0.7880873989823406 
epoch 49 loss 0.7919784495659982 
epoch 50 loss 0.8162226878180185 
epoch 51 loss 0.832984136486082 
epoch 52 loss 0.8015564202334631 
epoch 53 loss 0.822208919485184 
epoch 54 loss 0.8266985932355582 
epoch 55 loss 0.8356779407363065 
epoch 56 loss 0.8159233762346603 
epoch 57 loss 0.8302903322358576 
epoch 58 loss 0.8323855133193655 
epoch 59 loss 0.8413648608201137 
epoch 60 loss 0.829691709069141 
epoch 61 loss 0.8054474708171206 
epoch 62 loss 0.8260999700688416 
epoch 63 loss 0.8461538461538461 
epoch 64 loss 0.8410655492367555 
epoch 65 loss 0.8458545345704879 
epoch 66 loss 0.832984136486082 
epoch 67 loss 0.8491469619874289 
epoch 68 loss 0.8401676144866806 
epoch 69 loss 0.8434600419036217 
epoch 70 loss 0.8452559114037713 
epoch 71 loss 0.8485483388207123 
epoch 72 loss 0.8497455851541454 
epoch 73 loss 0.8389703681532475 
epoch 74 loss 0.8518407662376534 
epoch 75 loss 0.8521400778210116 
epoch 76 loss 0.8485483388207123 
epoch 77 loss 0.847051780903921 
epoch 78 loss 0.850344208320862 
epoch 79 loss 0.8428614187369051 
epoch 80 loss 0.8485483388207123 
epoch 81 loss 0.8617180484884766 
epoch 82 loss 0.853636635737803 
epoch 83 loss 0.8587249326548937 
epoch 84 loss 0.8620173600718348 
epoch 85 loss 0.8524393894043699 
epoch 86 loss 0.8581263094881771 
epoch 87 loss 0.8620173600718348 
epoch 88 loss 0.8653097874887759 
epoch 89 loss 0.8530380125710865 
epoch 90 loss 0.866507033822209 
epoch 91 loss 0.8440586650703382 
epoch 92 loss 0.8593235558216102 
epoch 93 loss 0.864411852738701 
epoch 94 loss 0.8650104759054176 
epoch 95 loss 0.8623166716551931 
epoch 96 loss 0.8671056569889255 
epoch 97 loss 0.8428614187369051 
epoch 98 loss 0.8736905118228075 
epoch 99 loss 0.8491469619874289 
epoch 100 loss 0.8689015264890751 
epoch 101 loss 0.864411852738701 
epoch 102 loss 0.873091888656091 
epoch 103 loss 0.8689015264890751 
epoch 104 loss 0.8772822508231068 
epoch 105 loss 0.8656090990721341 
epoch 106 loss 0.8659084106554924 
epoch 107 loss 0.856929063154744 
epoch 108 loss 0.866507033822209 
epoch 109 loss 0.8680035917390003 
epoch 110 loss 0.8703980844058665 
epoch 111 loss 0.8671056569889255 
epoch 112 loss 0.8614187369051183 
epoch 113 loss 0.8700987728225082 
epoch 114 loss 0.8659084106554924 
epoch 115 loss 0.8781801855731817 
epoch 116 loss 0.8715953307392996 
epoch 117 loss 0.8739898234061658 
epoch 118 loss 0.8841664172403472 
epoch 119 loss 0.8790781203232565 
epoch 120 loss 0.8754863813229572 
epoch 121 loss 0.8835677940736306 
epoch 122 loss 0.8706973959892248 
epoch 123 loss 0.8700987728225082 
epoch 124 loss 0.8787788087398982 
epoch 125 loss 0.8769829392397486 
epoch 126 loss 0.8671056569889255 
epoch 127 loss 0.8802753666566896 
epoch 128 loss 0.8647111643220593 
epoch 129 loss 0.8832684824902723 
epoch 130 loss 0.8823705477401975 
epoch 131 loss 0.8793774319066148 
epoch 132 loss 0.8727925770727327 
epoch 133 loss 0.8802753666566896 
epoch 134 loss 0.8829691709069141 
epoch 135 loss 0.867704280155642 
epoch 136 loss 0.8799760550733313 
epoch 137 loss 0.8727925770727327 
epoch 138 loss 0.8751870697395989 
epoch 139 loss 0.8671056569889255 
epoch 140 loss 0.864411852738701 
epoch 141 loss 0.8488476504040706 
epoch 142 loss 0.8772822508231068 
epoch 143 loss 0.867704280155642 
epoch 144 loss 0.8754863813229572 
epoch 145 loss 0.8802753666566896 
epoch 146 loss 0.8820712361568392 
epoch 147 loss 0.87847949715654 
epoch 148 loss 0.8811733014067644 
epoch 149 loss 0.8820712361568392
